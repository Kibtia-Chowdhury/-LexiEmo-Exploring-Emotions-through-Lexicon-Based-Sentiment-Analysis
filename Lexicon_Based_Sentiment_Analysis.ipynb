{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# LexiEmo: Exploring Emotions through Lexicon-Based Sentiment Analysis"
      ],
      "metadata": {
        "id": "4cQaYbEO_IoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('sentiwordnet')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "#remove annoying warnings from sklearn\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ],
      "metadata": {
        "id": "jPFXVR1FGeTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Load\n",
        "data = pd.read_csv('Twitter_Data.csv') # Load the CSV file\n",
        "text = data['clean_text'] # Extract the \"clean_text\"\n",
        "category = data['category'] # Extract the \"category\" columns"
      ],
      "metadata": {
        "id": "wkAeEf3vtOjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n",
        "        text = text.lower() # Convert to lowercase\n",
        "        tokens = word_tokenize(text) # Tokenize the text\n",
        "        stop_words = set(stopwords.words('english')) # Remove stopwords if necessary\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        text = ' '.join(tokens) # Join the tokens back into a single string\n",
        "    return text\n",
        "\n",
        "preprocessed_text = text.apply(preprocess_text)\n",
        "preprocessed_text = preprocessed_text[category.notnull()]\n",
        "category = category[category.notnull()]"
      ],
      "metadata": {
        "id": "t4T_7OGVtOmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each Label Count\n",
        "label_counts = data['category'].value_counts() # Calculate the count of each label\n",
        "print(\"Each Label Counts:\",label_counts)\n",
        "labels = ['Positive(1)', 'Neutral(0)', 'Negative(-1)'] # Define the labels for the donut chart\n",
        "mapped_labels = {1: 'Positive(1)', 0: 'Neutral(0)', -1: 'Negative(-1)'} # Map the labels based on the values\n",
        "print(\"Each Mapped Label:\",mapped_labels)\n",
        "mapped_label_counts = [label_counts[label] for label in mapped_labels.keys()] # Get the counts for the mapped labels\n",
        "light_colors = ['#FFBBBB', '#BBFFBB', '#BBBBFF'] # Define the light colors for each label\n",
        "\n",
        "plt.figure(figsize=(6, 4)) # Create a donut chart\n",
        "plt.pie(mapped_label_counts, labels=[f\"{mapped_labels[label]}: {count}\" for label, count in zip(mapped_labels.keys(), mapped_label_counts)],\n",
        "        autopct='%1.1f%%', startangle=90, colors=light_colors)\n",
        "\n",
        "center_circle = plt.Circle((0, 0), 0.6, color='white') # Add a circle at the center to create a donut shape\n",
        "plt.gca().add_artist(center_circle)\n",
        "plt.title('Distribution of Tweets by Labels') # Add a title\n",
        "plt.show() # Display the donut chart"
      ],
      "metadata": {
        "id": "7x6ZHSWutOqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_text, category, test_size=0.2, random_state=42)\n",
        "print(\"X Train Shape:\",X_train.shape)\n",
        "print(\"X Test Shape:\",X_test.shape)\n",
        "print(\"Y Train Shape:\",y_train.shape)\n",
        "print(\"Y Test Shape:\",y_test.shape)"
      ],
      "metadata": {
        "id": "e6t58ftKtepN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlob Sentiment Analysis\n",
        "blob_polarity = [TextBlob(text).sentiment.polarity if isinstance(text, str) else None for text in X_train]\n",
        "blob_predictions = [1 if polarity and polarity > 0 else -1 if polarity and polarity < 0 else 0 for polarity in blob_polarity]\n",
        "blob_accuracy = accuracy_score(y_train, blob_predictions)\n",
        "blob_precision = precision_score(y_train, blob_predictions, average='macro')\n",
        "blob_recall = recall_score(y_train, blob_predictions, average='macro')\n",
        "blob_f1 = f1_score(y_train, blob_predictions, average='macro')\n",
        "blob_confusion_matrix = confusion_matrix(y_train, blob_predictions)\n",
        "\n",
        "print(\"TextBlob Accuracy:\",blob_accuracy)\n",
        "print(\"TextBlob Precision:\",blob_precision)\n",
        "print(\"TextBlob Recall:\",blob_recall)\n",
        "print(\"TextBlob F1:\",blob_f1)\n",
        "print(\"TextBlob Confusion Matrix:\",blob_confusion_matrix)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "labels = ['Positive', 'Neutral', 'Negative'] # Define the labels for the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 4)) # Plot the confusion matrix\n",
        "im = ax.imshow(blob_confusion_matrix, cmap='Greens')\n",
        "cbar = ax.figure.colorbar(im, ax=ax) # Set the colorbar\n",
        "ax.set(xticks=np.arange(blob_confusion_matrix.shape[1]), # Set the labels\n",
        "       yticks=np.arange(blob_confusion_matrix.shape[0]),\n",
        "       xticklabels=labels, yticklabels=labels,\n",
        "       title='TextBlob Confusion Matrix',\n",
        "       ylabel='True label',\n",
        "       xlabel='Predicted label')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", # Rotate the x-axis labels\n",
        "         rotation_mode=\"anchor\")\n",
        "for i in range(blob_confusion_matrix.shape[0]): # Loop over the data and create text annotations\n",
        "    for j in range(blob_confusion_matrix.shape[1]):\n",
        "        ax.text(j, i, format(blob_confusion_matrix[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if blob_confusion_matrix[i, j] > np.max(blob_confusion_matrix) / 2 else \"black\")\n",
        "plt.show() # Show the plot"
      ],
      "metadata": {
        "id": "TC8_y32qtere"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SentiWordNet Sentiment Analysis\n",
        "def calculate_sentiment_score(text):\n",
        "    pos_score = 0\n",
        "    neg_score = 0\n",
        "    for word in word_tokenize(text):\n",
        "        synsets = list(swn.senti_synsets(word))\n",
        "        if synsets:\n",
        "            synset = synsets[0]\n",
        "            pos_score += synset.pos_score()\n",
        "            neg_score += synset.neg_score()\n",
        "    return pos_score - neg_score\n",
        "\n",
        "swn_polarity = [calculate_sentiment_score(text) if isinstance(text, str) else None for text in X_train]\n",
        "swn_predictions = [1 if polarity and polarity > 0 else -1 if polarity and polarity < 0 else 0 for polarity in swn_polarity]\n",
        "swn_accuracy = accuracy_score(y_train, swn_predictions)\n",
        "swn_precision = precision_score(y_train, swn_predictions, average='macro')\n",
        "swn_recall = recall_score(y_train, swn_predictions, average='macro')\n",
        "swn_f1 = f1_score(y_train, swn_predictions, average='macro')\n",
        "swn_confusion_matrix = confusion_matrix(y_train, swn_predictions)\n",
        "\n",
        "print(\"SentiWordNet Accuracy:\",swn_accuracy)\n",
        "print(\"SentiWordNet Precision:\",swn_precision)\n",
        "print(\"SentiWordNet Recall:\",swn_recall)\n",
        "print(\"SentiWordNet F1:\",swn_f1)\n",
        "print(\"SentiWordNet Confusion Matrix:\",swn_confusion_matrix)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "labels = ['Positive', 'Neutral', 'Negative'] # Define the labels for the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 4)) # Plot the confusion matrix\n",
        "im = ax.imshow(swn_confusion_matrix, cmap='Blues')\n",
        "cbar = ax.figure.colorbar(im, ax=ax) # Set the colorbar\n",
        "ax.set(xticks=np.arange(swn_confusion_matrix.shape[1]), # Set the labels\n",
        "       yticks=np.arange(swn_confusion_matrix.shape[0]),\n",
        "       xticklabels=labels, yticklabels=labels,\n",
        "       title='SentiWordNet Confusion Matrix',\n",
        "       ylabel='True label',\n",
        "       xlabel='Predicted label')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", # Rotate the x-axis labels\n",
        "         rotation_mode=\"anchor\")\n",
        "for i in range(swn_confusion_matrix.shape[0]): # Loop over the data and create text annotations\n",
        "    for j in range(swn_confusion_matrix.shape[1]):\n",
        "        ax.text(j, i, format(swn_confusion_matrix[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if swn_confusion_matrix[i, j] > np.max(swn_confusion_matrix) / 2 else \"black\")\n",
        "plt.show() # Show the plot"
      ],
      "metadata": {
        "id": "ovp1dUVTtlnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "vader_polarity = [sid.polarity_scores(text)['compound'] if isinstance(text, str) else 0 for text in X_train]\n",
        "vader_predictions = [1 if polarity > 0 else -1 if polarity < 0 else 0 for polarity in vader_polarity]\n",
        "vader_accuracy = accuracy_score(y_train, vader_predictions)\n",
        "vader_precision = precision_score(y_train, vader_predictions, average='macro')\n",
        "vader_recall = recall_score(y_train, vader_predictions, average='macro')\n",
        "vader_f1 = f1_score(y_train, vader_predictions, average='macro')\n",
        "vader_confusion_matrix = confusion_matrix(y_train, vader_predictions)\n",
        "\n",
        "print(\"VADER Accuracy:\",vader_accuracy)\n",
        "print(\"VADER Precision:\",vader_precision)\n",
        "print(\"VADER Recall:\",vader_recall)\n",
        "print(\"VADER F1:\",vader_f1)\n",
        "print(\"VADER Confusion Matrix:\",vader_confusion_matrix)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "labels = ['Positive', 'Neutral', 'Negative'] # Define the labels for the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 4)) # Plot the confusion matrix\n",
        "im = ax.imshow(vader_confusion_matrix, cmap='Oranges')\n",
        "cbar = ax.figure.colorbar(im, ax=ax) # Set the colorbar\n",
        "ax.set(xticks=np.arange(vader_confusion_matrix.shape[1]), # Set the labels\n",
        "       yticks=np.arange(vader_confusion_matrix.shape[0]),\n",
        "       xticklabels=labels, yticklabels=labels,\n",
        "       title='VADER Confusion Matrix',\n",
        "       ylabel='True label',\n",
        "       xlabel='Predicted label')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", # Rotate the x-axis labels\n",
        "         rotation_mode=\"anchor\")\n",
        "for i in range(vader_confusion_matrix.shape[0]): # Loop over the data and create text annotations\n",
        "    for j in range(vader_confusion_matrix.shape[1]):\n",
        "        ax.text(j, i, format(vader_confusion_matrix[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if vader_confusion_matrix[i, j] > np.max(vader_confusion_matrix) / 2 else \"black\")\n",
        "plt.show() # Show the plot"
      ],
      "metadata": {
        "id": "Yv_8lxkRteuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NB Model Training\n",
        "X_train_cleaned = np.where(pd.isnull(X_train), \"\", X_train) # Replace missing values with an empty string\n",
        "vectorizer = TfidfVectorizer() # Fit and transform the vectorizer\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train_cleaned)\n",
        "# Split the preprocessed data into training and testing sets\n",
        "X_train_model, X_test_model, y_train_model, y_test_model = train_test_split(X_train_vectorized, y_train, test_size=0.2, random_state=42)\n",
        "nb_classifier = MultinomialNB() # Train a Naive Bayes classifier\n",
        "nb_classifier.fit(X_train_model, y_train_model)\n",
        "nb_predictions = nb_classifier.predict(X_test_model) # Evaluate the trained model on the testing data\n",
        "nb_accuracy = accuracy_score(y_test_model, nb_predictions)\n",
        "nb_precision = precision_score(y_test_model, nb_predictions, average='macro')\n",
        "nb_recall = recall_score(y_test_model, nb_predictions, average='macro')\n",
        "nb_f1 = f1_score(y_test_model, nb_predictions, average='macro')\n",
        "nb_confusion_matrix = confusion_matrix(y_test_model, nb_predictions)\n",
        "\n",
        "print(\"Naive Bayes Accuracy:\",nb_accuracy)\n",
        "print(\"Naive Bayes Precision:\",nb_precision)\n",
        "print(\"Naive Bayes Recall:\",nb_recall)\n",
        "print(\"Naive Bayes F1:\",nb_f1)\n",
        "print(\"Naive Bayes Confusion Matrix:\",nb_confusion_matrix)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "labels = ['Positive', 'Neutral', 'Negative'] # Define the labels for the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 4)) # Plot the confusion matrix\n",
        "im = ax.imshow(nb_confusion_matrix, cmap='YlOrRd')\n",
        "cbar = ax.figure.colorbar(im, ax=ax) # Set the colorbar\n",
        "ax.set(xticks=np.arange(nb_confusion_matrix.shape[1]), # Set the labels\n",
        "       yticks=np.arange(nb_confusion_matrix.shape[0]),\n",
        "       xticklabels=labels, yticklabels=labels,\n",
        "       title='Naive Bayes Confusion Matrix',\n",
        "       ylabel='True label',\n",
        "       xlabel='Predicted label')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", # Rotate the x-axis labels\n",
        "         rotation_mode=\"anchor\")\n",
        "for i in range(nb_confusion_matrix.shape[0]): # Loop over the data and create text annotations\n",
        "    for j in range(nb_confusion_matrix.shape[1]):\n",
        "        ax.text(j, i, format(nb_confusion_matrix[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if nb_confusion_matrix[i, j] > np.max(nb_confusion_matrix) / 2 else \"black\")\n",
        "plt.show() # Show the plot"
      ],
      "metadata": {
        "id": "t83L_VT1tOuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Each Methods Label\n",
        "methods = ['TextBlob', 'SentiWordNet', 'VADER', 'Trained Model']\n",
        "label_counts_per_method = []\n",
        "blob_label_counts = pd.Series(blob_predictions).value_counts().reindex([-1, 0, 1], fill_value=0)\n",
        "label_counts_per_method.append(blob_label_counts)\n",
        "swn_label_counts = pd.Series(swn_predictions).value_counts().reindex([-1, 0, 1], fill_value=0)\n",
        "label_counts_per_method.append(swn_label_counts)\n",
        "vader_label_counts = pd.Series(vader_predictions).value_counts().reindex([-1, 0, 1], fill_value=0)\n",
        "label_counts_per_method.append(vader_label_counts)\n",
        "model_label_counts = pd.Series(nb_predictions).value_counts().reindex([-1, 0, 1], fill_value=0)\n",
        "label_counts_per_method.append(model_label_counts)\n",
        "\n",
        "print(\"BlobText:\",blob_label_counts)\n",
        "print(\"SentiWordNet:\",swn_label_counts)\n",
        "print(\"Vader:\",vader_label_counts)\n",
        "print(\"NaiveBayes:\",model_label_counts)\n",
        "\n",
        "custom_colors = ['#FF69B4', '#8A2BE2', '#FFA500', '#00CED1'] # Define custom colors for each sentiment label\n",
        "plt.figure(figsize=(6, 4)) # Plot the stacked bar chart with custom colors\n",
        "x = np.arange(len(labels))\n",
        "width = 0.6\n",
        "bottom = np.zeros(len(labels))\n",
        "for i, method in enumerate(methods):\n",
        "    plt.bar(x, label_counts_per_method[i], width=width, bottom=bottom, label=method, color=custom_colors[i % len(custom_colors)])\n",
        "    bottom += label_counts_per_method[i]\n",
        "\n",
        "plt.xlabel('Sentiment Labels')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(x, labels)\n",
        "plt.title('Composition of Sentiment Labels by Methods')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HiILlah7t1IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Results\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "blob_scores = [blob_accuracy, blob_precision, blob_recall, blob_f1]\n",
        "swn_scores = [swn_accuracy, swn_precision, swn_recall, swn_f1]\n",
        "vader_scores = [vader_accuracy, vader_precision, vader_recall, vader_f1]\n",
        "model_scores = [nb_accuracy, nb_precision, nb_recall, nb_f1]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "x = range(len(metrics))\n",
        "plt.bar(x, blob_scores, width=0.2, label='TextBlob')\n",
        "plt.bar([i + 0.2 for i in x], swn_scores, width=0.2, label='SentiWordNet')\n",
        "plt.bar([i + 0.4 for i in x], vader_scores, width=0.2, label='VADER')\n",
        "plt.bar([i + 0.6 for i in x], model_scores, width=0.2, label='Trained Model')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Scores')\n",
        "plt.xticks([i + 0.3 for i in x], metrics)\n",
        "plt.title('Comparison of Sentiment Analysis Approaches')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "79iAyibX2jil"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}